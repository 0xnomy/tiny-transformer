model:
  d_model: 256
  n_heads: 8
  n_encoder_layers: 4
  n_decoder_layers: 4
  d_ff: 1024
  dropout: 0.1
  max_seq_length: 50

training:
  batch_size: 8
  learning_rate: 0.0003
  num_epochs: 100
  warmup_steps: 2000
  label_smoothing: 0.05
  grad_clip: 1.0
  checkpoint_dir: "checkpoints"
  patience: 10

data:
  vocab_size: 2000
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1